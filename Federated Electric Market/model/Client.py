import math

import torch
import copy
import numpy as np
from tqdm import tqdm

from model.models import BiLSTM
from torch import nn
from torch.optim.lr_scheduler import StepLR

from utils.parameter_tran import parameter_to_str, str_to_parameter, get_shape_list
from utils.rsa_algo import rsaEncrypt
from utils.aes_algo import aesDecrypt, aesEncrypt
from sklearn.metrics import mean_absolute_error
import math

# æ„å»ºè”é‚¦å­¦ä¹ çš„å®¢æˆ·ç«¯
class client():
    def __init__(self, args, train_data, val_data, test_data, max_load, min_load, rsa_public_k, aes_k):
        self.args = args
        self.model = BiLSTM(args).to(args.device)
        self.train_data = train_data
        self.val_data = val_data
        self.test_data = test_data
        self.max_load = max_load
        self.min_load = min_load
        self.rsa_public_k = rsa_public_k
        self.aes_k = aes_k

    def update_local_model(self, c):
        m = aesDecrypt(c, self.aes_k)
        shape_list = get_shape_list(self.model)
        params = str_to_parameter(m, shape_list, self.args.round)
        model_params = list(self.model.parameters())
        # å°†æ‰€æœ‰çš„model_paramsè¿›è¡Œå‡å€¼å¤„ç†
        for i in range(len(model_params)):
            model_params[i].data = params[i]

    # def LDP(self, tensor):
    #     tensor_mean = torch.abs(torch.mean(tensor))
    #     tensor = torch.clamp(tensor, min=-self.args.clip, max=self.args.clip)
    #     noise = torch.distributions.laplace.Laplace(0, tensor_mean * self.args.laplace_lambda).sample()
    #     tensor += noise
    #     return tensor

    def train(self, num):
        print('client {} is training '.format(num))
        train_data = self.train_data

        # å®šä¹‰æŸå¤±å‡½æ•°MSE
        loss_function = nn.MSELoss().to(self.args.device)
        # å®šä¹‰ä¼˜åŒ–å™¨
        if self.args.optimizer == 'adam':
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr,
                                         weight_decay=self.args.weight_decay)
        else:
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.args.lr,
                                        momentum=0.9, weight_decay=self.args.weight_decay)

        scheduler = StepLR(optimizer, step_size=self.args.step_size, gamma=self.args.gamma)

        # æ€»è®­ç»ƒè¯¯å·®
        tol_loss = []

        for epoch in range(self.args.local_epochs):
            train_loss = []
            for (seq, label) in (train_data):
                seq, label = seq.to(self.args.device), label.to(self.args.device)
                self.model.zero_grad()
                y_pred = self.model(seq)
                loss = loss_function(y_pred, label)
            # è¯¯å·®æŒ‡æ ‡éƒ¨åˆ†ğŸ‘‡
                loss_mse = loss.item()
                loss_mae = mean_absolute_error([y.item() for y in label],[y.item() for y in y_pred])
                loss_rmse = math.sqrt(loss_mse)
                train_loss.append([loss_mse,loss_mae,loss_rmse])
            # è¯¯å·®æŒ‡æ ‡éƒ¨åˆ†ğŸ‘†
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            # ä¸ºæœ€åçš„æ¢¯åº¦ä¿¡æ¯åŠ ä¸ŠLaplaceå™ªå£°åä¸Šä¼ 
            tol_loss.append(np.sum(train_loss,axis=0)/len(train_loss))
            # ä½¿ç”¨gammaè°ƒæ•´å­¦ä¹ ç‡
            scheduler.step()

            self.model.train()

        # å¯¹æ‰€æœ‰çš„æ¨¡å‹å‚æ•°åˆ©ç”¨rsaå…¬é’¥è¿›è¡ŒåŠ å¯†å¹¶ä¸Šä¼ 
        # model_params = list(self.model.parameters())
        # m = parameter_to_str(model_params, self.args.round)
        # c = rsaEncrypt(m, self.rsa_public_k, self.args.round)

        # åˆ©ç”¨aesåŠ å¯†ï¼ˆèŠ‚çœæ—¶é—´ï¼‰
        model_params = list(self.model.parameters())
        m = bytes(parameter_to_str(model_params, self.args.round), 'utf8')
        c = aesEncrypt(m, self.aes_k)

        return c, np.sum(tol_loss,axis=0)/len(tol_loss) #sum(tol_loss) / len(tol_loss)
        # return model_list, sum(tol_loss) / len(tol_loss)
